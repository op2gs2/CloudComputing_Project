{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nCaptum\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \ud574\uc11d\ud558\uae30\n===================================\n\n**\ubc88\uc5ed**: `\uc815\uc7ac\ubbfc <https://github.com/jjeamin>`_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum\uc744 \uc0ac\uc6a9\ud558\uba74 \ub370\uc774\ud130 \ud2b9\uc9d5(features)\uc774 \ubaa8\ub378\uc758 \uc608\uce21 \ub610\ub294 \ub274\ub7f0 \ud65c\uc131\ud654\uc5d0\n\ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc774\ud574\ud558\uace0, \ubaa8\ub378\uc758 \ub3d9\uc791 \ubc29\uc2dd\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uadf8\ub9ac\uace0 \\ ``Integrated Gradients``\\ \uc640 \\ ``Guided GradCam``\\ \uacfc \uac19\uc740\n\ucd5c\ucca8\ub2e8\uc758 feature attribution \uc54c\uace0\ub9ac\uc998\uc744 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 Captum\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc74c\uc744 \uc218\ud589\ud558\ub294 \ubc29\ubc95\uc744 \ubc30\uc6c1\ub2c8\ub2e4:\n\n- \uc774\ubbf8\uc9c0 \ubd84\ub958\uae30(classifier)\uc758 \uc608\uce21\uc744 \ud574\ub2f9 \uc774\ubbf8\uc9c0\uc758 \ud2b9\uc9d5(features)\uc5d0 \ud45c\uc2dc\ud558\uae30\n- \uc18d\uc131(attribution) \uacb0\uacfc\ub97c \uc2dc\uac01\ud654 \ud558\uae30\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc2dc\uc791\ud558\uae30 \uc804\uc5d0\n----------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum\uc774 Python \ud658\uacbd\uc5d0 \uc124\uce58\ub418\uc5b4 \uc788\ub294\uc9c0 \ud655\uc778\ud574\uc57c \ud569\ub2c8\ub2e4.\nCaptum\uc740 Github\uc5d0\uc11c ``pip`` \ud328\ud0a4\uc9c0 \ub610\ub294 ``conda`` \ud328\ud0a4\uc9c0\ub85c \uc81c\uacf5\ub429\ub2c8\ub2e4.\n\uc790\uc138\ud55c \uc9c0\uce68\uc740 https://captum.ai/ \uc758 \uc124\uce58 \uc548\ub0b4\uc11c\ub97c \ucc38\uc870\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378\uc758 \uacbd\uc6b0, PyTorch\uc5d0 \ub0b4\uc7a5 \ub41c \uc774\ubbf8\uc9c0 \ubd84\ub958\uae30(classifier)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\nCaptum\uc740 \uc0d8\ud50c \uc774\ubbf8\uc9c0\uc758 \uc5b4\ub5a4 \ubd80\ubd84\uc774 \ubaa8\ub378\uc5d0 \uc758\ud574 \ub9cc\ub4e4\uc5b4\uc9c4\n\ud2b9\uc815\ud55c \uc608\uce21\uc5d0 \ub3c4\uc6c0\uc744 \uc8fc\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nmodel = torchvision.models.resnet18(pretrained=True).eval()\n\nresponse = requests.get(\"https://image.freepik.com/free-photo/two-beautiful-puppies-cat-dog_58409-6024.jpg\")\nimg = Image.open(BytesIO(response.content))\n\ncenter_crop = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n])\n\nnormalize = transforms.Compose([\n    transforms.ToTensor(),               # \uc774\ubbf8\uc9c0\ub97c 0\uc5d0\uc11c 1\uc0ac\uc774\uc758 \uac12\uc744 \uac00\uc9c4 Tensor\ub85c \ubcc0\ud658\n    transforms.Normalize(                # 0\uc744 \uc911\uc2ec\uc73c\ub85c \ud558\ub294 imagenet \ud53d\uc140\uc758 rgb \ubd84\ud3ec\ub97c \ub530\ub974\ub294 \uc815\uaddc\ud654\n     mean=[0.485, 0.456, 0.406],\n     std=[0.229, 0.224, 0.225]\n    )\n])\ninput_img = normalize(center_crop(img)).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc18d\uc131(attribution) \uacc4\uc0b0\ud558\uae30\n---------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378\uc758 top-3 \uc608\uce21 \uc911\uc5d0\ub294 \uac1c\uc640 \uace0\uc591\uc774\uc5d0 \ud574\ub2f9\ud558\ub294 \ud074\ub798\uc2a4 208\uacfc 283\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\nCaptum\uc758 \\ ``Occlusion``\\ \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac01 \uc608\uce21\uc744 \uc785\ub825\uc758 \ud574\ub2f9 \ubd80\ubd84\uc5d0 \ud45c\uc2dc\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from captum.attr import Occlusion\n\nocclusion = Occlusion(model)\n\nstrides = (3, 9, 9)               # \uc791\uc744\uc218\ub85d = \uc138\ubd80\uc801\uc778 \uc18d\uc131\uc774\uc9c0\ub9cc \ub290\ub9bc\ntarget=208,                       # ImageNet\uc5d0\uc11c Labrador\uc758 \uc778\ub371\uc2a4\nsliding_window_shapes=(3,45, 45)  # \uac1d\uccb4\uc758 \ubaa8\uc591\uc744 \ubcc0\ud654\uc2dc\ud0a4\uae30\uc5d0 \ucda9\ubd84\ud55c \ud06c\uae30\ub97c \uc120\ud0dd\nbaselines = 0                     # \uc774\ubbf8\uc9c0\ub97c \uac00\ub9b4 \uac12, 0\uc740 \ud68c\uc0c9\n\nattribution_dog = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=baselines)\n\n\ntarget=283,                       # ImageNet\uc5d0\uc11c Persian cat\uc758 \uc778\ub371\uc2a4\nattribution_cat = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum\uc740 ``Occlusion`` \uc678\uc5d0\ub3c4 \\ ``Integrated Gradients``\\ , \\ ``Deconvolution``\\ ,\n\\ ``GuidedBackprop``\\ , \\ ``Guided GradCam``\\ , \\ ``DeepLift``\\ ,\n\uadf8\ub9ac\uace0 \\ ``GradientShap``\\\uacfc \uac19\uc740 \ub9ce\uc740 \uc54c\uace0\ub9ac\uc998\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\uc774\ub7ec\ud55c \ubaa8\ub4e0 \uc54c\uace0\ub9ac\uc998\uc740 \ucd08\uae30\ud654\ud560 \ub54c \ubaa8\ub378\uc744 \ud638\ucd9c \uac00\ub2a5\ud55c \\ ``forward_func``\\ \uc73c\ub85c \uae30\ub300\ud558\uba70\n\uc18d\uc131(attribution) \uacb0\uacfc\ub97c \ud1b5\ud569\ud574\uc11c \ubc18\ud658\ud558\ub294 ``attribute(...)`` \uba54\uc18c\ub4dc\ub97c \uac00\uc9c0\ub294\n``Attribution`` \uc758 \uc11c\ube0c\ud074\ub798\uc2a4 \uc785\ub2c8\ub2e4.\n\n\uc774\ubbf8\uc9c0\uc778 \uacbd\uc6b0 \uc18d\uc131(attribution) \uacb0\uacfc\ub97c \uc2dc\uac01\ud654 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uacb0\uacfc \uc2dc\uac01\ud654\ud558\uae30\n-----------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum\uc758 \\ ``visualization``\\ \uc720\ud2f8\ub9ac\ud2f0\ub294 \uadf8\ub9bc\uacfc \ud14d\uc2a4\ud2b8 \uc785\ub825 \ubaa8\ub450\uc5d0 \ub300\ud55c\n\uc18d\uc131(attribution) \uacb0\uacfc\ub97c \uc2dc\uac01\ud654 \ud560 \uc218 \uc788\ub294 \uc989\uc2dc \uc0ac\uc6a9\uac00\ub2a5\ud55c \ubc29\ubc95\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom captum.attr import visualization as viz\n\n# \uacc4\uc0b0 \uc18d\uc131 Tensor\ub97c \uc774\ubbf8\uc9c0 \uac19\uc740 numpy \ubc30\uc5f4\ub85c \ubcc0\ud658\ud569\ub2c8\ub2e4.\nattribution_dog = np.transpose(attribution_dog.squeeze().cpu().detach().numpy(), (1,2,0))\n\nvis_types = [\"heat_map\", \"original_image\"]\nvis_signs = [\"all\", \"all\"] # \"positive\", \"negative\", \ub610\ub294 \ubaa8\ub450 \ud45c\uc2dc\ud558\ub294 \"all\"\n# positive \uc18d\uc131\uc740 \ud574\ub2f9 \uc601\uc5ed\uc758 \uc874\uc7ac\uac00 \uc608\uce21 \uc810\uc218\ub97c \uc99d\uac00\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.\n# negative \uc18d\uc131\uc740 \ud574\ub2f9 \uc601\uc5ed\uc758 \uc874\uc7ac\uac00 \uc608\uce21 \uc810\uc218\ub97c \ub0ae\ucd94\ub294 \uc624\ub2f5 \uc601\uc5ed\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.\n\n_ = viz.visualize_image_attr_multiple(attribution_dog,\n                                      np.array(center_crop(img)),\n                                      vis_types,\n                                      vis_signs,\n                                      [\"attribution for dog\", \"image\"],\n                                      show_colorbar = True\n                                     )\n\n\nattribution_cat = np.transpose(attribution_cat.squeeze().cpu().detach().numpy(), (1,2,0))\n\n_ = viz.visualize_image_attr_multiple(attribution_cat,\n                                      np.array(center_crop(img)),\n                                      [\"heat_map\", \"original_image\"],\n                                      [\"all\", \"all\"], # positive/negative \uc18d\uc131 \ub610\ub294 all\n                                      [\"attribution for cat\", \"image\"],\n                                      show_colorbar = True\n                                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub9cc\uc57d \ub370\uc774\ud130\uac00 \ud14d\uc2a4\ud2b8\uc778 \uacbd\uc6b0 ``visualization.visualize_text()`` \ub294\n\uc785\ub825 \ud14d\uc2a4\ud2b8 \uc704\uc5d0 \uc18d\uc131(attribution)\uc744 \ud0d0\uc0c9\ud560 \uc218 \uc788\ub294 \uc804\uc6a9 \ubdf0(view)\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.\nhttp://captum.ai/tutorials/IMDB_TorchText_Interpret \uc5d0\uc11c \uc790\uc138\ud55c \ub0b4\uc6a9\uc744 \ud655\uc778\ud558\uc138\uc694.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub9c8\uc9c0\ub9c9 \ub178\ud2b8\n-----------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Captum\uc740 \uc774\ubbf8\uc9c0, \ud14d\uc2a4\ud2b8 \ub4f1\uc744 \ud3ec\ud568\ud558\uc5ec \ub2e4\uc591\ud55c \ubc29\uc2dd\uc73c\ub85c PyTorch\uc5d0\uc11c \ub300\ubd80\ubd84\uc758 \ubaa8\ub378 \ud0c0\uc785\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nCaptum\uc744 \uc0ac\uc6a9\ud558\uba74 \ub2e4\uc74c\uc744 \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\\* \uc704\uc5d0\uc11c \uc124\uba85\ud55c \uac83\ucc98\ub7fc \ud2b9\uc815\ud55c \ucd9c\ub825\uc744 \ubaa8\ub378 \uc785\ub825\uc5d0 \ud45c\uc2dc\ud558\uae30\n\\* \ud2b9\uc815\ud55c \ucd9c\ub825\uc744 \uc740\ub2c9\uce35\uc758 \ub274\ub7f0\uc5d0 \ud45c\uc2dc\ud558\uae30 (Captum API reference\ub97c \ubcf4\uc138\uc694).\n\\* \ubaa8\ub378 \uc785\ub825\uc5d0 \ub300\ud55c \uc740\ub2c9\uce35 \ub274\ub7f0\uc758 \ubc18\uc751\uc744 \ud45c\uc2dc\ud558\uae30 (Captum API reference\ub97c \ubcf4\uc138\uc694).\n\n\uc9c0\uc6d0\ub418\ub294 \uba54\uc18c\ub4dc\uc758 \uc804\uccb4 API\uc640 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ubaa9\ub85d\uc740 http://captum.ai \ub97c \ucc38\uc870\ud558\uc138\uc694.\n\nGilbert Tanner\uc758 \ub610 \ub2e4\ub978 \uc720\uc6a9\ud55c \uac8c\uc2dc\ubb3c :\nhttps://gilberttanner.com/blog/interpreting-pytorch-models-with-captum\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}